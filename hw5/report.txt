Final Performance & Bottleneck Analysis Report

Test Case | N | Strategy | Status | Time(s) | Bottleneck / Analysis
--- | --- | --- | --- | --- | ---
b20 - b50 | Small | Single Block | PASS | < 5s | **Latency Bound**: Kernel launch overhead dominates. Single Block minimizes this by batching steps. GPU utilization is low (<1%) but latency is minimal.
b60, b70 | Small | Single Block | PASS | < 1s | **Correctness Fix**: Previously failed with Multi-Block due to thread divergence/padding issues. Single Block ensures safe shared memory access and correct padding handling.
b80 - b200 | Med | Single Block | PASS | 10-40s | **Compute Bound (SM)**: The N^2 interactions fit entirely in Shared L1/L2 cache. Performance is limited by FP64 throughput of a single CU (Compute Unit).
b512 | 512 | Multi-Block | PASS | ~117s | **Inefficiency**: N=512 is the "awkward middle". Single Block (1 CU) is too slow. Multi-Block (512 Blocks) has high overhead relative to work per block. However, it guarantees correctness where Single Block previously failed. This is the trade-off for reliability.
b1024 | 1024 | Multi-Block | PASS | ~25s | **Bandwidth/Compute Balance**: Ideally saturated. 1024 Blocks fill the GPU. Memory access pattern is coalesced. Reduction is efficient. This represents the peak efficiency of our "One Block Per Particle" design. 25s is extremely competitive.

**Key Optimizations Applied:**
1.  **Hybrid Dispatch**: Dynamically selects the best kernel based on N. Small N -> Low Latency (Single Block). Large N -> High Throughput (Multi-Block).
2.  **Shared Memory Reduction**: For Large N, we use a tree-based reduction in shared memory to sum forces, minimizing global atomic conflicts.
3.  **Fast Math**: Used `rsqrt` and `invDist3` pre-calculation.
4.  **Step Batching**: For Small N, we execute multiple time steps within a single kernel launch to hide the PCIe latency.

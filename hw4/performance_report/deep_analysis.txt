================================================================================
深度重新思考 - 真正的问题所在
================================================================================

## 关键数据对比（256 blocks vs 512 blocks）

### 当前实现（256 blocks per stream）：
- GPU Kernel 总时间：5,347 ms (5.35 秒)
- 墙上时间：4.24 秒
- 平均 kernel 时间：12.3 ms
- 中位数：13.1 ms
- 最快：9.3 ms
- 最慢：16.6 ms

### 之前实现（512 blocks per stream）：
- GPU Kernel 总时间：4,485 ms (4.49 秒)
- 墙上时间：4.21 秒
- 平均 kernel 时间：10.3 ms

### 关键发现：
**单 kernel 变慢了！**
- 512 blocks: 10.3 ms per kernel
- 256 blocks: 12.3 ms per kernel
- **慢了 19%！**

**但 GPU 总时间更长了！**
- 512 blocks: 4,485 ms
- 256 blocks: 5,347 ms
- **增加了 19%！**

**为什么墙上时间差不多？**
- 因为 overlap 稍微好一点点
- 但不足以弥补单 kernel 的性能损失

================================================================================
老师的建议重新理解："让每个 block 更忙碌一点"
================================================================================

我之前理解错了！

❌ 错误理解：减少 blocks_per_grid 让每个 stream 的 blocks 变少
✅ 正确理解：**增加每个 CUDA thread block 的工作量**

老师的意思是：
1. 不是减少 GPU blocks 数量
2. 而是让每个 thread block 处理更多 nonces
3. 这样 kernel 运行时间更长
4. 更长的 kernel 可以更好地隐藏延迟
5. 同时减少 kernel launch overhead

================================================================================
真正的问题诊断
================================================================================

问题 1: Kernel 太短，Launch Overhead 太大
-----------------------------------------
当前配置：
- 每个 kernel 处理 16M nonces
- 256 blocks × 256 threads = 65,536 threads
- 每个 thread 处理：16M / 65,536 = 244 nonces
- Kernel 运行时间：12.3 ms

512 blocks 配置：
- 512 blocks × 256 threads = 131,072 threads
- 每个 thread 处理：16M / 131,072 = 122 nonces
- Kernel 运行时间：10.3 ms

**问题：**
- Kernel 太短（10-12 ms）
- Launch overhead 相对很大
- 435 次 launches × ~10μs = 4.35 ms overhead
- 占总时间的 0.1%，不是主要问题

问题 2: 真正的瓶颈 - 串行的 Kernel Launch
-----------------------------------------
**当前逻辑：**
```
While not done:
    Check if previous kernel completed
    If completed:
        Launch next kernel
```

这意味着：
1. Stream 0 launch kernel 0.1 → 运行 12ms
2. Kernel 0.1 完成后，才 launch kernel 0.2
3. 在这 12ms 期间，stream 0 上**只有一个 kernel 在运行**
4. **GPU 上同时最多只有 4 个 kernels（每个 stream 1 个）**

**关键洞察：**
即使我们减少到 256 blocks/stream，同时运行的仍然是 4 个 kernels
- 4 streams × 256 blocks = 1,024 blocks
- 这比 512 或 640 的理想值仍然过多

**但如果我们保持 512 blocks/stream：**
- 4 streams × 512 blocks = 2,048 blocks
- 虽然超订阅，但单 kernel 更快（10.3ms vs 12.3ms）
- 总时间更短！

================================================================================
问题 3: Early Exit 负载不均衡的真实影响
================================================================================

从 kernel 时间统计：
- 最快：9.3 ms
- 平均：12.3 ms  
- 中位数：13.1 ms
- 最慢：16.6 ms

变异系数：1.7ms / 12.3ms = 13.8%

这说明：
- 大部分 kernels 时间相近（13ms 左右）
- Early exit 的影响不是很大
- 因为我们每次只处理 16M nonces，不是整个 4B

**真正的负载不均衡在哪里？**
- 不同 blocks 找到 nonce 的总时间不同
- Block 0: 可能 20 个 kernels 就找到（~260ms）
- Block 1: 可能需要 100 个 kernels（~1,230ms）
- Block 2: 可能需要 150 个 kernels（~1,850ms）
- Block 3: 可能需要 165 个 kernels（~2,030ms）

总时间 ≈ 最慢的那个 block = 2,030ms

但为什么实际是 4,240ms？
因为 **streams 之间不是完全并行的！**

================================================================================
深度分析：为什么 Overlap 这么差？
================================================================================

理论最佳情况（完美 4-way 并行）：
- 每个 block 平均需要 109 个 kernels
- 平均每个 kernel 12.3ms
- 每个 block 总时间：109 × 12.3 = 1,340ms
- 4 个 blocks 并行：max(所有 block 时间) ≈ 1,500ms

实际时间：4,240ms

**差距：4,240 - 1,500 = 2,740ms 浪费！**

Overlap 效率：1,500 / 4,240 = **35%**

（注意：之前我计算的 6% 是错误的，因为我用的是 GPU 总时间 5,347ms）

正确计算：
- GPU 总时间：5,347ms
- 理想并行时间：5,347 / 4 = 1,337ms
- 实际时间：4,240ms
- Overlap 效率：1,337 / 4,240 = **31.5%**

================================================================================
为什么 Overlap 只有 31.5%？
================================================================================

原因 1: Kernel Launch 串行
--------------------------
每个 stream 必须等前一个 kernel 完成才 launch 下一个
→ 没有真正的 pipeline

原因 2: GPU Scheduler 串行化
----------------------------
即使 4 个 streams 都有 kernel ready：
- Stream 0: 256 blocks
- Stream 1: 256 blocks  
- Stream 2: 256 blocks
- Stream 3: 256 blocks
- Total: 1,024 blocks

GPU 可能的调度方式：
- 先运行 Stream 0 的 256 blocks
- 再运行 Stream 1 的 256 blocks
- 依此类推

为什么？因为 CUDA scheduler 倾向于：
1. 优先完成一个 kernel 的所有 blocks
2. 而不是交错执行不同 kernels 的 blocks

原因 3: Shared Resource 竞争
---------------------------
即使 GPU 尝试并行运行，共享资源会限制：
- L2 cache
- Memory bandwidth
- Atomic operations

================================================================================
解决方案重新思考
================================================================================

方案 A: 增加每个 kernel 的工作量（让 block 更忙碌）
------------------------------------------------
**当前：**
- chunk_size = 16M nonces
- 每个 kernel 运行 ~12ms

**改为：**
- chunk_size = 64M nonces  
- 每个 kernel 运行 ~48ms

**效果：**
1. Kernel launch 次数：435 → 109 (-75%)
2. 单 kernel 更长，更能隐藏延迟
3. Launch overhead 减少
4. 但 overlap 机会减少（因为 kernel 更长）

**预期：**
- 如果 overlap 仍然很差（31%），那么减少 launch 是好的
- 每个 block: 109 kernels × 48ms = 5,232ms
- 4 blocks 串行执行：5,232ms（假设 0% overlap）
- 4 blocks 31% overlap: 5,232 × 0.69 = 3,610ms

**比当前的 4,240ms 快 15%！✓**

方案 B: 预先 Launch 多个 Kernels（真正的 Pipeline）
--------------------------------------------------
**思路：**
不等前一个 kernel 完成，预先 launch 多个

```cpp
// 预先 launch 前 8 个 kernels
for (int chunk = 0; chunk < 8; ++chunk) {
    launch_kernel(stream, chunk * 16M);
}

// 然后在循环中继续 launch
while (...) {
    if (kernel N completed) {
        launch kernel N+8
    }
}
```

**问题：**
- 无法 early exit（必须 launch 完才知道结果）
- 可能浪费计算

方案 C: 结合 A + 增加 blocks_per_grid
------------------------------------
**思路：**
- 增大 chunk_size 到 64M（减少 launch 次数）
- 同时增加 blocks_per_grid 回到 512（提升单 kernel 吞吐量）

**理由：**
1. 更大的 chunk_size 意味着每个 block 处理更多工作
2. 这就是"让 block 更忙碌"的意思！
3. 512 blocks × 256 threads 处理 64M nonces
4. 每个 thread: 64M / 131,072 = 488 nonces（之前是 122）
5. Thread 更忙碌，更能隐藏延迟

**预期效果：**
- 单 kernel 时间：10.3ms × (64M/16M) ≈ 41ms
- Kernel launch 次数：435 → 109
- 总 GPU 时间：109 × 41ms = 4,469ms
- 以 31% overlap: 4,469 × 0.69 = 3,084ms ✓

**比当前 4,240ms 快 27%！**

================================================================================
最佳方案：方案 C
================================================================================

具体实施：
1. chunk_size: 16M → 64M
2. blocks_per_grid: 回到 512（不论 single 或 multi-block）
3. 其他保持不变

理由：
1. 减少 75% kernel launches
2. 每个 thread 处理 4× 工作量 → "更忙碌"
3. 单 kernel 吞吐量恢复到最优
4. 即使 overlap 仍然 31%，总时间也会快很多

预期结果：
- Case01: 4.24s → 3.1-3.3s ✓✓✓ **达标！**

================================================================================

================================================================================
最终性能分析 - chunk_size 64M 的影响
================================================================================

## 关键数据对比

### 配置 1: chunk_size = 16M, blocks = 256
- GPU 总时间：5,347 ms
- Kernel launches：435 次
- 平均 kernel 时间：12.3 ms
- 墙上时间：4.24 秒

### 配置 2: chunk_size = 16M, blocks = 512 (原始)
- GPU 总时间：4,485 ms
- Kernel launches：435 次
- 平均 kernel 时间：10.3 ms
- 墙上时间：4.21 秒

### 配置 3: chunk_size = 64M, blocks = 512 (当前)
- GPU 总时间：4,556 ms ✓
- Kernel launches：110 次 ✓✓✓ (减少 75%!)
- 平均 kernel 时间：41.4 ms
- 中位数：42.1 ms
- 最快：37.2 ms
- 最慢：53.6 ms
- 墙上时间：4.24 秒

================================================================================
分析结果
================================================================================

好消息：
-------
✅ Kernel launches 减少了 75% (435 → 110)
✅ 每个 block 更忙碌：处理 64M 而不是 16M
✅ GPU 总时间略有改善：4,556ms vs 4,485ms (+1.6%)
✅ 单 kernel 时间符合预期：41.4ms ≈ 10.3ms × 4

坏消息：
-------
⚠️  墙上时间没有改善：仍然 4.24 秒
⚠️  Overlap 效率仍然很差

为什么墙上时间没有改善？
-----------------------

理论分析：
- GPU 总时间：4,556 ms
- 理想 4-way 并行：4,556 / 4 = 1,139 ms
- 实际墙上时间：4,240 ms
- Overlap 效率：1,139 / 4,240 = 26.9%

**关键洞察：Overlap 效率变差了！**
- 16M chunk: 31.5% overlap
- 64M chunk: 26.9% overlap
- **下降了 4.6 个百分点！**

为什么 Overlap 变差了？
----------------------

原因：**更长的 kernel 意味着更少的调度机会**

16M chunk 情况：
- 每个 stream 有 ~109 个 kernels
- GPU scheduler 有 109 次机会交错执行不同 streams
- 即使每次只能串行，但有更多"切换点"

64M chunk 情况：
- 每个 stream 只有 ~27 个 kernels  
- GPU scheduler 只有 27 次机会交错执行
- 更少的"切换点"导致更差的 overlap

数学证明：
----------
设：
- N = kernel launches per stream
- T = single kernel time
- O = overlap efficiency

Total GPU time = N × T
Wall time = (N × T) × (1 - O) / num_streams + (N × T) × O

简化为：
Wall time = (N × T) / (num_streams × (1 + (num_streams-1) × O))

对于 4 streams：
Wall time = (N × T) / (4 × (1 + 3×O))

16M: N=109, T=10.3ms, O=0.315
Wall time = (109 × 10.3) / (4 × 1.945) = 1,122.7 / 7.78 = 144ms... 等等这不对

让我重新计算...

实际观察到的现象：
-----------------
GPU 总时间几乎不变：
- 16M × 435 kernels = 4,485ms
- 64M × 110 kernels = 4,556ms
- 差异只有 1.6%

墙上时间也几乎不变：
- 16M: 4.21s
- 64M: 4.24s
- 差异只有 0.7%

**结论：Overlap 效率被 kernel 粒度抵消了**

================================================================================
真正的瓶颈：串行的 Kernel Launch Logic
================================================================================

无论 chunk size 是 16M 还是 64M，主循环逻辑都是：

```cpp
while (completed_count < tasks.size()) {
    // Check completed
    for (int i = 0; i < 4; ++i) {
        if (cudaEventQuery(contexts[i].event) == cudaSuccess) {
            // Process result
        }
    }
    
    // Launch next chunk
    for (int i = 0; i < 4; ++i) {
        if (kernel_completed[i]) {
            launch_next_kernel(...);
        }
    }
}
```

**关键问题：每个 stream 同时只有 1 个 kernel 在运行！**

即使我们有 4 个 streams，GPU 上同时最多只有：
- 4 kernels × 512 blocks = 2,048 blocks

但这些 2,048 blocks 不是真正并行的，因为：
1. CUDA scheduler 倾向于先完成一个 kernel
2. 不同 kernels 的 blocks 不会完全交错执行

结果：大部分时间只有 1-2 个 kernels 真正在运行

================================================================================
为什么这个问题无法通过调整参数解决？
================================================================================

根本原因：架构限制

当前架构：
```
For each stream:
    Wait for previous kernel to complete
    Launch next kernel
```

这是一个**反应式（Reactive）架构**：
- 等待完成 → 反应 → Launch
- 天生串行

需要的是**主动式（Proactive）架构**：
```
For each stream:
    Pre-launch多个 kernels
    让 CUDA runtime 自动调度
```

但这有问题：
- 无法 early exit
- 必须 launch 所有 kernels 才能知道是否找到

================================================================================
实验结论
================================================================================

1. **增加 chunk_size 不能改善性能**
   - 因为 overlap 效率下降抵消了 launch overhead 的改善

2. **减少 blocks_per_grid 也不能改善性能**
   - 因为单 kernel 吞吐量下降大于 overlap 的提升

3. **真正的瓶颈是架构性的**
   - 串行的 kernel launch 逻辑
   - 这需要重新设计整个 pipeline

4. **当前最佳配置**
   - blocks_per_grid = 512 (最大吞吐量)
   - chunk_size = 16M (平衡 launch overhead 和 overlap)
   - 结果：4.21 秒

================================================================================
能达到目标 3.0-3.5s 吗？
================================================================================

从数据分析：

GPU 总时间：4,485 ms (无法再优化，已经是最快的 kernel)
Overlap 效率：31.5% (架构限制，很难再提升)

理想情况（100% overlap）：
4,485 / 4 = 1,121 ms

要达到 3,500 ms 需要的 overlap：
1,121 / 3,500 = 32% ✓✓✓ **我们已经有 31.5%！**

**等等！**

让我重新计算...

如果 GPU 总时间是 4,485ms，墙上时间是 4,210ms
那说明有部分 overlap 让墙上时间小于 GPU 总时间

但 GPU 总时间 > 墙上时间 意味着有些 kernels 是并行的

正确的计算方式：
- 如果完全串行：墙上时间 = GPU 总时间 = 4,485ms
- 实际墙上时间：4,210ms
- 节省时间：4,485 - 4,210 = 275ms (6.1%)

这 275ms 是通过并行节省的。

要达到 3,500ms：
需要节省：4,485 - 3,500 = 985ms
当前节省：275ms
还需要：710ms (增加 2.6×)

**这意味着需要把 overlap 从目前的 6% 提升到 22%**

可能吗？
-------
**很难！**

因为当前架构下，4 个 streams 已经在尽力并行了
要提升 3.6×，需要：
1. 真正的 pipeline (预先 launch 多个 kernels)
2. 或减少 blocks 让 GPU 能真正并行运行多个 kernels
   - 但我们已经试过，单 kernel 会变慢更多

结论：
-----
在当前架构下，很难达到 3.0-3.5s 的目标。

需要：
1. 重新设计 pipeline 架构（预先 launch）
2. 或进行 SHA-256 kernel 级别优化（减少 GPU 总时间）
3. 或两者结合

================================================================================
最终建议
================================================================================

建议 1：接受当前性能 4.21s
- 比初始版本快 2×
- 代码稳定可靠
- 所有测试通过

建议 2：尝试预先 launch (高风险)
- 可能达到 3.5-3.8s
- 但无法 early exit
- 可能浪费计算

建议 3：SHA-256 kernel 优化 (极高难度)
- 需要汇编级优化
- 可能达到 3.2-3.5s
- 需要深厚的 GPU 编程经验

================================================================================
